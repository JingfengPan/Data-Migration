import pandas as pd
import numpy as np
from sentence_transformers import SentenceTransformer, util
import os
import csv
import time

# initialize centroids
def random_centroids(data, k):
    centroids = []
    for i in range(k):
        centroid = data.apply(lambda x: float(x.sample())) # randomly choose a value in each column
        centroids.append(centroid)
    return pd.concat(centroids, axis=1) # combine the pandas series to dataframe
    
# Label func
def get_labels(data, centroids):
    distances = centroids.apply(lambda x: np.sqrt(((data - x)**2).sum(axis=1)))
    return distances.idxmin(axis=1)
    
# Label func for each data instance
def get_label(data, centroids):
    distance = centroids.apply(lambda x: np.sqrt(((data - x)**2).sum()))
    return int(distance.idxmin())
    
#get new centroids
def new_centroids(data, labels):
    return data.groupby(labels).apply(lambda x: np.exp(np.log(x).mean())).T 
    
chunksize=500
reader = pd.read_csv("DS001_10000rows.csv",chunksize=chunksize)
data = pd.read_csv("DS001_10000rows.csv")
data

#max_iteration = 100
k = 10
chunk_num = 0
model = SentenceTransformer('all-MiniLM-L6-v2')

#max size for output
max_size = 300
output = []
for i in range(k):
    output.append([])

#index for name the files
output_index = []
for i in range(k):
    output_index.append(0)

#index for getting the data
row_index = 0

for chunk in reader:
    # Bert transform
    corpuss = (chunk['Content'].values.astype("U").tolist())
    corpuss_embeddings = model.encode(corpuss)
    
    # change the type to pandas dataframe
    df = pd.DataFrame(data = corpuss_embeddings[0:,0:])
    #df = df.dropna()
    df = ((df - df.min())/(df.max() - df.min())) * 9 + 1

    # random initialize centroids the first time
    if chunk_num == 0:
        centroids = random_centroids(df, k)
        chunk_num += 1
          
    
    labels_list = []
    for index, row in df.iteritems():
        # get the label of the row
        label = get_label(row, centroids)
        labels_list.append(label)
        r = row_index+index
        output[label].append(data.iloc[(r),])
        #print(output[label])
        
        #check if the cluster is full
        if len(output[label]) == max_size:
            output_file = pd.concat(output[label])
            file_name = 'cluster' + str(label) + '_' + str(output_index[label]) + '.csv'
            output_file.to_csv(file_name, index=False, header=False)
            
            #print(output[label])
            output[label].clear()
            output_index[label] += 1
            
    #update centroids    
    labels = pd.Series(labels_list)
    centroids = new_centroids(df, labels)
    
    #update row index
    row_index += chunksize

for i in range(k):
    output_file = pd.concat(output[i])
    file_name = 'cluster' + str(i) + '_' + str(output_index[i]) + '.csv'
    output_file.to_csv(file_name, index=False, header=False)
